{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 1 - Gemini API\n",
    "This path will guide you on the set-up and usage of Google Gemini's API.\n",
    "\n",
    "### 0. Get and store the API key\n",
    "First of all, you need to login with your google account and get an API key [here](https://aistudio.google.com/app/apikey). It is **very important** that you do not share your API key with anyone and that you do not have it in your Repository.\n",
    "\n",
    "You can keep your API key in a secure local document and access it when needed. It is common to save the key as an environmental variable so that it can be accessed by your python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this means your API key is in plain text in your script. To avoid this, if you're using VS Code, you can add your API key to a `.env` file in your workspace root with the following line:\n",
    "\n",
    "```sh\n",
    "GEMINI_API_KEY=\"PASTE YOUR KEY HERE\"\n",
    "```\n",
    "\n",
    "Alternatively, you can use the [dot-env library](https://github.com/theskumar/python-dotenv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$GEMINI_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "# You can check if the environment variable API_KEY has been set up properly by running this line\n",
    "!if [ -z $GEMINI_API_KEY ]; then echo \"\\$GEMINI_API_KEY not found\"; else echo \"\\$GEMINI_API_KEY found\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple request\n",
    "Now, you can write a simple script to see if everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.genai.client.Client at 0x7ad0c4433fe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()  # here you can also pass the api_key directly using os.environ['GEMINI_API_KEY']\n",
    "\n",
    "default_model = \"gemini-2.5-flash\"\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Ask the model to generate content about a random topic and print the response in text.\n",
    "\n",
    "Here is the [official documentation](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#configure) to find the help you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is one of the most profound, complex, and deeply human experiences, incredibly difficult to capture in a single, definitive statement. It's universally felt yet uniquely experienced by each individual.\n",
      "\n",
      "Here's an attempt to break down its multifaceted nature:\n",
      "\n",
      "1.  **A Deep Emotion and Connection:**\n",
      "    *   At its core, love is a strong feeling of affection, care, and attachment towards someone or something.\n",
      "    *   It involves a sense of deep connection, belonging, and a bond that can transcend time and space.\n",
      "\n",
      "2.  **A Set of Behaviors and Actions:**\n",
      "    *   Love is not just a feeling; it's also a **choice** and a **verb**. It manifests through actions like:\n",
      "        *   **Care and Support:** Looking out for another's well-being, offering help, being present.\n",
      "        *   **Empathy and Understanding:** Trying to see the world through another's eyes, listening, validating feelings.\n",
      "        *   **Commitment and Loyalty:** Standing by someone, even through difficulties, and showing faithfulness.\n",
      "        *   **Sacrifice and Giving:** Prioritizing another's needs, sometimes at one's own expense.\n",
      "        *   **Forgiveness and Acceptance:** Overlooking flaws, accepting imperfections, and letting go of resentment.\n",
      "        *   **Respect and Admiration:** Valuing someone for who they are, their qualities, and their journey.\n",
      "\n",
      "3.  **Different Forms and Expressions:**\n",
      "    *   **Romantic Love (Eros):** Characterized by passion, intimacy, desire, and commitment. It often involves a deep longing for union and emotional intensity.\n",
      "    *   **Familial Love (Storge):** The unconditional bond between family members (parents, children, siblings). It's often built on shared history, loyalty, and deep affection.\n",
      "    *   **Platonic Love (Philia):** The strong bond of friendship, based on mutual respect, shared interests, trust, and companionship.\n",
      "    *   **Self-Love:** Essential for well-being, it's about valuing oneself, self-acceptance, self-care, and having a healthy sense of self-worth.\n",
      "    *   **Universal Love (Agape):** A selfless, unconditional, altruistic love for humanity, nature, or a higher power. It's compassion and benevolence towards all.\n",
      "    *   **Love for Hobbies/Passions:** A deep enthusiasm and devotion to an activity, art, or ideal.\n",
      "\n",
      "4.  **Biological, Psychological, and Philosophical Dimensions:**\n",
      "    *   **Biologically:** Love triggers the release of hormones like oxytocin (bonding), dopamine (pleasure/reward), and vasopressin (attachment), influencing our feelings and behaviors. It plays a role in human survival and procreation.\n",
      "    *   **Psychologically:** Love is linked to attachment theory, emotional regulation, and cognitive processes that shape our perceptions of others and ourselves.\n",
      "    *   **Philosophically/Spiritually:** Love is often seen as a fundamental aspect of human existence, giving life meaning, fostering connection, and sometimes transcending the material world.\n",
      "\n",
      "5.  **A Source of Both Joy and Vulnerability:**\n",
      "    *   Love brings immense joy, comfort, peace, excitement, and a sense of purpose. It can make life feel vibrant and meaningful.\n",
      "    *   However, it also makes us vulnerable to pain, heartbreak, fear of loss, and disappointment. To love is to open oneself up to potential suffering.\n",
      "\n",
      "**In essence:**\n",
      "\n",
      "Love is a dynamic, evolving force that fuels connection, growth, and meaning in our lives. It's a spectrum of emotions, a commitment to action, and a fundamental aspect of what it means to be human. It's something we strive for, nurture, and often spend a lifetime trying to understand and embody.\n",
      "\n",
      "Ultimately, what love *is* for you will be a deeply personal and ongoing discovery.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=default_model,\n",
    "    contents=\"What is love?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is an overview of the parameters that Gemini offers. Try some of them in different context and understand how they affect the final generated text.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too). Consider keeping the `max_output_tokens` to 50 so that the output is not too long; if you do, you should also set a low `thinking_budget` to avoid an empty response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic question, and it's something many people are curious about! The term \"AI\" is a broad umbrella, so let's break down the core concepts and different ways it works.\n",
      "\n",
      "At its heart, **AI\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "        temperature=1,\n",
    "        maxOutputTokens=50\n",
    "    ),\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic question, and the answer is both incredibly complex and surprisingly simple at its core. Let's break down how AI works, starting with the basics and moving to more advanced concepts.\n",
      "\n",
      "## The Core Idea: Learning from Data\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "        temperature=0,\n",
    "        maxOutputTokens=50,\n",
    "        topK=50\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens *after* applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic question, and the answer is both complex and surprisingly simple at its core. Let's break down how AI works, starting with the basics and moving to more advanced concepts.\n",
      "\n",
      "## The Core Idea: Learning from Data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "        temperature=0.7,\n",
    "        maxOutputTokens=50,\n",
    "        topK=50,\n",
    "        topP=0.1\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the topK to 1 is like setting a low temperature because then it will only take the most likely token. The same goes for a low topP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "\n",
    "#### Exercise 5\n",
    "Gemini, beside text also accepts images (and videos). Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the [official documentation](https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-images).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"./data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents and include it as additional context. This becomes especially relevant if you have many documents that cannot possibly fit into the model's context window. To more easily implement a RAG pipeline we recommend the use of one of these libraries: [LangChain](https://python.langchain.com/v0.2/docs/introduction/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/), [Haystack](https://docs.haystack.deepset.ai/docs/intro).\n",
    "\n",
    "For the solution of this lab we will use *LangChain*.\n",
    "\n",
    "It can be useful to split this exercise into these steps:\n",
    "1. Read one or more documents using pdfminer\n",
    "2. Split the documents into small chunks\n",
    "3. Get and store the embeddings for each chunks\n",
    "5. Given a query, retrieve the most relevant chunk(s) and appropriately prompt your LLM\n",
    "\n",
    "**NOTE:** if you try to embed too many documents at once or too large documents you may run into rate limits. Possible solutions: \n",
    "* Reduce the number of chunks and/or their size\n",
    "* Look at the HF version of this lab and use a local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # langchain expects gemini's api key to be in the environment variable GOOGLE_API_KEY, use os to set it\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # get embeddings from Gemini\n",
    "from langchain_community.vectorstores import FAISS  # \"db\" to store and retrieve embeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # split long documents\n",
    "from pdfminer.high_level import extract_text  # extract text from pdfs\n",
    "\n",
    "DOC_PATH = \"./data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "# Suppose a user query\n",
    "USER_QUERY = \"What is CoT?\"\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore on your own\n",
    "Gemini offers a bigger range of capabilities than those provided here, begin able to automatically handle multi-turn chats is one of them. Explore them on your own!\n",
    "\n",
    "#### Exercise 7\n",
    "Explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a user interface\n",
    "\n",
    "#### Exercise 8\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface).\n",
    "\n",
    "Use a ChatInterface to create a chatbot UI that let's you discuss with Gemini, then add multimodal capabilities for both Gradio and Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Edit the parameters below\n",
    "chats = {}  # store the chat history for each user (suppose multiple users)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
