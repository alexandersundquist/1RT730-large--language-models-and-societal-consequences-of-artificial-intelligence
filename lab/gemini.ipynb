{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 1 - Gemini API\n",
    "This path will guide you on the set-up and usage of Google Gemini's API.\n",
    "\n",
    "### 0. Get and store the API key\n",
    "First of all, you need to login with your google account and get an API key [here](https://aistudio.google.com/app/apikey). It is **very important** that you do not share your API key with anyone and that you do not have it in your Repository.\n",
    "\n",
    "You can keep your API key in a secure local document and access it when needed. It is common to save the key as an environmental variable so that it can be accessed by your python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this means your API key is in plain text in your script. To avoid this, if you're using VS Code, you can add your API key to a `.env` file in your workspace root with the following line:\n",
    "\n",
    "```sh\n",
    "GEMINI_API_KEY=\"PASTE YOUR KEY HERE\"\n",
    "```\n",
    "\n",
    "Alternatively, you can use the [dot-env library](https://github.com/theskumar/python-dotenv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$GOOGLE_API_KEY found\n",
      "$GEMINI_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "# You can check if the environment variable API_KEY has been set up properly by running this line\n",
    "!if [ -z $GOOGLE_API_KEY ]; then echo \"\\$GOOGLE_API_KEY not found\"; else echo \"\\$GOOGLE_API_KEY found\"; fi\n",
    "!if [ -z $GEMINI_API_KEY ]; then echo \"\\$GEMINI_API_KEY not found\"; else echo \"\\$GEMINI_API_KEY found\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple request\n",
    "Now, you can write a simple script to see if everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.genai.client.Client at 0x7ad0c4433fe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()  # here you can also pass the api_key directly using os.environ['GEMINI_API_KEY']\n",
    "\n",
    "default_model = \"gemini-2.5-flash\"\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Ask the model to generate content about a random topic and print the response in text.\n",
    "\n",
    "Here is the [official documentation](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#configure) to find the help you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is one of the most profound, complex, and deeply human experiences, incredibly difficult to capture in a single, definitive statement. It's universally felt yet uniquely experienced by each individual.\n",
      "\n",
      "Here's an attempt to break down its multifaceted nature:\n",
      "\n",
      "1.  **A Deep Emotion and Connection:**\n",
      "    *   At its core, love is a strong feeling of affection, care, and attachment towards someone or something.\n",
      "    *   It involves a sense of deep connection, belonging, and a bond that can transcend time and space.\n",
      "\n",
      "2.  **A Set of Behaviors and Actions:**\n",
      "    *   Love is not just a feeling; it's also a **choice** and a **verb**. It manifests through actions like:\n",
      "        *   **Care and Support:** Looking out for another's well-being, offering help, being present.\n",
      "        *   **Empathy and Understanding:** Trying to see the world through another's eyes, listening, validating feelings.\n",
      "        *   **Commitment and Loyalty:** Standing by someone, even through difficulties, and showing faithfulness.\n",
      "        *   **Sacrifice and Giving:** Prioritizing another's needs, sometimes at one's own expense.\n",
      "        *   **Forgiveness and Acceptance:** Overlooking flaws, accepting imperfections, and letting go of resentment.\n",
      "        *   **Respect and Admiration:** Valuing someone for who they are, their qualities, and their journey.\n",
      "\n",
      "3.  **Different Forms and Expressions:**\n",
      "    *   **Romantic Love (Eros):** Characterized by passion, intimacy, desire, and commitment. It often involves a deep longing for union and emotional intensity.\n",
      "    *   **Familial Love (Storge):** The unconditional bond between family members (parents, children, siblings). It's often built on shared history, loyalty, and deep affection.\n",
      "    *   **Platonic Love (Philia):** The strong bond of friendship, based on mutual respect, shared interests, trust, and companionship.\n",
      "    *   **Self-Love:** Essential for well-being, it's about valuing oneself, self-acceptance, self-care, and having a healthy sense of self-worth.\n",
      "    *   **Universal Love (Agape):** A selfless, unconditional, altruistic love for humanity, nature, or a higher power. It's compassion and benevolence towards all.\n",
      "    *   **Love for Hobbies/Passions:** A deep enthusiasm and devotion to an activity, art, or ideal.\n",
      "\n",
      "4.  **Biological, Psychological, and Philosophical Dimensions:**\n",
      "    *   **Biologically:** Love triggers the release of hormones like oxytocin (bonding), dopamine (pleasure/reward), and vasopressin (attachment), influencing our feelings and behaviors. It plays a role in human survival and procreation.\n",
      "    *   **Psychologically:** Love is linked to attachment theory, emotional regulation, and cognitive processes that shape our perceptions of others and ourselves.\n",
      "    *   **Philosophically/Spiritually:** Love is often seen as a fundamental aspect of human existence, giving life meaning, fostering connection, and sometimes transcending the material world.\n",
      "\n",
      "5.  **A Source of Both Joy and Vulnerability:**\n",
      "    *   Love brings immense joy, comfort, peace, excitement, and a sense of purpose. It can make life feel vibrant and meaningful.\n",
      "    *   However, it also makes us vulnerable to pain, heartbreak, fear of loss, and disappointment. To love is to open oneself up to potential suffering.\n",
      "\n",
      "**In essence:**\n",
      "\n",
      "Love is a dynamic, evolving force that fuels connection, growth, and meaning in our lives. It's a spectrum of emotions, a commitment to action, and a fundamental aspect of what it means to be human. It's something we strive for, nurture, and often spend a lifetime trying to understand and embody.\n",
      "\n",
      "Ultimately, what love *is* for you will be a deeply personal and ongoing discovery.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=default_model,\n",
    "    contents=\"What is love?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is an overview of the parameters that Gemini offers. Try some of them in different context and understand how they affect the final generated text.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too). Consider keeping the `max_output_tokens` to 50 so that the output is not too long; if you do, you should also set a low `thinking_budget` to avoid an empty response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic question, and it's something many people are curious about! The term \"AI\" is a broad umbrella, so let's break down the core concepts and different ways it works.\n",
      "\n",
      "At its heart, **AI\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "        temperature=1,\n",
    "        maxOutputTokens=50\n",
    "    ),\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic question, and the answer is both incredibly complex and surprisingly simple at its core. Let's break down how AI works, starting with the basics and moving to more advanced concepts.\n",
      "\n",
      "## The Core Idea: Learning from Data\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "        temperature=0,\n",
    "        maxOutputTokens=50,\n",
    "        topK=50\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens *after* applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic question, and the answer is both complex and surprisingly simple at its core. Let's break down how AI works, starting with the basics and moving to more advanced concepts.\n",
      "\n",
      "## The Core Idea: Learning from Data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "        temperature=0.7,\n",
    "        maxOutputTokens=50,\n",
    "        topK=50,\n",
    "        topP=0.1\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the topK to 1 is like setting a low temperature because then it will only take the most likely token. The same goes for a low topP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "\n",
    "#### Exercise 5\n",
    "Gemini, beside text also accepts images (and videos). Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the [official documentation](https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-images).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options for a caption, ranging in detail:\n",
      "\n",
      "**Concise:**\n",
      "*   Two men, both with advanced prosthetic limbs, collaborate on adjusting a bionic arm in a modern rehabilitation setting.\n",
      "\n",
      "**Descriptive:**\n",
      "*   In a bright, modern facility, a man with multiple limb prostheses, including a bionic arm, sits in a wheelchair while another man, also with a prosthetic arm, carefully adjusts the bionic limb. The scene suggests a fitting, training, or calibration session for advanced assistive technology.\n",
      "\n",
      "**Detailed:**\n",
      "*   A powerful image of advanced rehabilitation or research, showing one man in a wheelchair, fitted with bionic arms and leg prostheses, receiving focused assistance from another man, who also uses a sophisticated prosthetic arm. They are intently adjusting the bionic arm, likely during a fitting, programming, or training session for the cutting-edge device.\n",
      "\n",
      "**Focus on Collaboration/Support:**\n",
      "*   Embodying the spirit of innovation and peer support in prosthetics, two men with artificial limbs work together to fine-tune a bionic arm for a patient in a wheelchair within a clinical environment.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "IMAGE_PATH = \"../data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "\n",
    "# Your code here\n",
    "im = Image.open(IMAGE_PATH)\n",
    "#display(im)\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "with open(IMAGE_PATH, 'rb') as f:\n",
    "  image_bytes = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model='gemini-2.5-flash',\n",
    "contents=[\n",
    "  types.Part.from_bytes(\n",
    "    data=image_bytes,\n",
    "    mime_type='image/jpeg',\n",
    "  ),\n",
    "  'Caption this image.'\n",
    "]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few caption options for the image, focusing on different aspects:\n",
      "\n",
      "**Option 1 (Detailed & Comprehensive):**\n",
      "In a bright, modern clinic or lab, a man in a wheelchair, equipped with advanced prosthetic arms and legs, has his upper limb prosthesis meticulously adjusted by another man. The assisting individual also appears to be wearing a prosthetic glove on his left hand, suggesting a shared understanding of prosthetic technology.\n",
      "\n",
      "**Option 2 (Concise, highlighting collaboration):**\n",
      "Two men, both users of advanced prosthetic technology, collaborate in a contemporary lab setting. One in a wheelchair has his bionic arm examined and adjusted by the other, who also has a visible prosthetic hand.\n",
      "\n",
      "**Option 3 (Focus on rehabilitation/progress):**\n",
      "A man in a wheelchair, with multiple advanced prosthetics, receives expert assistance with his bionic arm during a session in a modern rehabilitation facility. The person providing assistance also utilizes a prosthetic hand.\n",
      "\n",
      "**Option 4 (Simple & Direct):**\n",
      "A man in a wheelchair with prosthetic limbs receives adjustment and support for his bionic arm from another individual, who also has a prosthetic hand, in a well-lit indoor environment.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import io\n",
    "\n",
    "IMAGE_PATH = \"../data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "\n",
    "# Load image with PIL\n",
    "im = Image.open(IMAGE_PATH)\n",
    "\n",
    "# Optional: preprocess with PIL (resize, convert, etc.)\n",
    "# im = im.convert(\"RGB\")\n",
    "# im = im.resize((512, 512))\n",
    "\n",
    "# Convert PIL image into bytes in memory\n",
    "buffer = io.BytesIO()\n",
    "im.save(buffer, format=\"JPEG\")   # or \"PNG\" depending on your needs\n",
    "image_bytes = buffer.getvalue()\n",
    "\n",
    "# Display in notebook (optional)\n",
    "# display(im)\n",
    "\n",
    "# Initialize client\n",
    "client = genai.Client()\n",
    "\n",
    "# Send to API\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents=[\n",
    "        types.Part.from_bytes(\n",
    "            data=image_bytes,\n",
    "            mime_type='image/jpeg',  # match the format you used in im.save()\n",
    "        ),\n",
    "        'Caption this image.'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents and include it as additional context. This becomes especially relevant if you have many documents that cannot possibly fit into the model's context window. To more easily implement a RAG pipeline we recommend the use of one of these libraries: [LangChain](https://python.langchain.com/v0.2/docs/introduction/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/), [Haystack](https://docs.haystack.deepset.ai/docs/intro).\n",
    "\n",
    "For the solution of this lab we will use *LangChain*.\n",
    "\n",
    "It can be useful to split this exercise into these steps:\n",
    "1. Read one or more documents using pdfminer\n",
    "2. Split the documents into small chunks\n",
    "3. Get and store the embeddings for each chunks\n",
    "5. Given a query, retrieve the most relevant chunk(s) and appropriately prompt your LLM\n",
    "\n",
    "**NOTE:** if you try to embed too many documents at once or too large documents you may run into rate limits. Possible solutions: \n",
    "* Reduce the number of chunks and/or their size\n",
    "* Look at the HF version of this lab and use a local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758832622.380524     185 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1758832622.387413     185 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os  # langchain expects gemini's api key to be in the environment variable GOOGLE_API_KEY, use os to set it\n",
    "import time\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # get embeddings from Gemini\n",
    "from langchain_community.vectorstores import FAISS  # \"db\" to store and retrieve embeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # split long documents\n",
    "import logging\n",
    "# lower pdfminer logging so those messages don't appear\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "from pdfminer.high_level import extract_text  # extract text from pdfs\n",
    "\n",
    "DOC_PATH = \"../data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "# Suppose a user query\n",
    "USER_QUERY = \"What is CoT?\"\n",
    "\n",
    "# Load text\n",
    "pdf_text = extract_text(DOC_PATH)\n",
    "#print(pdf_text[:100])\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,     # Number of characters per chunk\n",
    "    chunk_overlap=200,   # Overlap between chunks to preserve context\n",
    "    length_function=len  # How chunk size is measured (default is len)\n",
    ")\n",
    "\n",
    "# Split the text into LangChain Document objects\n",
    "chunks = text_splitter.split_documents(\n",
    "    [Document(page_content=pdf_text, metadata={'source': DOC_PATH})]\n",
    ")\n",
    "\n",
    "'''\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f'--- Chunk {i+1} ---')\n",
    "    print(chunk.page_content[:300]) \n",
    "    print()\n",
    "'''\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Initialize Gemini embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create FAISS vectorstore from the chunks\n",
    "#vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save the FAISS index so it can be reloaded later\n",
    "#vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758833592.821245     185 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1758833592.824497     185 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/langchain_google_genai/embeddings.py:243\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_embed_contents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mBatchEmbedContentsRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1437\u001b[39m, in \u001b[36mGenerativeServiceClient.batch_embed_contents\u001b[39m\u001b[34m(self, request, model, requests, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m batch = chunks[i:i + batch_size]\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Embed and create FAISS index for this batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m batch_vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vectorstore \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     50\u001b[39m     vectorstore = batch_vectorstore\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:837\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    835\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/langchain_google_genai/embeddings.py:247\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    243\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.client.batch_embed_contents(\n\u001b[32m    244\u001b[39m             BatchEmbedContentsRequest(requests=requests, model=\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    248\u001b[39m     embeddings.extend([\u001b[38;5;28mlist\u001b[39m(e.values) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result.embeddings])\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "import os  # langchain expects gemini's api key to be in the environment variable GOOGLE_API_KEY, use os to set it\n",
    "import time\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # get embeddings from Gemini\n",
    "from langchain_community.vectorstores import FAISS  # \"db\" to store and retrieve embeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # split long documents\n",
    "import logging\n",
    "# lower pdfminer logging so those messages don't appear\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "from pdfminer.high_level import extract_text  # extract text from pdfs\n",
    "\n",
    "DOC_PATH = \"../data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "# Suppose a user query\n",
    "USER_QUERY = \"What is CoT?\"\n",
    "\n",
    "# Load text\n",
    "pdf_text = extract_text(DOC_PATH)\n",
    "#print(pdf_text[:100])\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,     # Number of characters per chunk\n",
    "    chunk_overlap=200,   # Overlap between chunks to preserve context\n",
    "    length_function=len  # How chunk size is measured (default is len)\n",
    ")\n",
    "\n",
    "# Split the text into LangChain Document objects\n",
    "chunks = text_splitter.split_documents(\n",
    "    [Document(page_content=pdf_text, metadata={'source': DOC_PATH})]\n",
    ")\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Initialize Gemini embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create FAISS vectorstore from the chunks\n",
    "vectorstore = None  # start empty\n",
    "\n",
    "# Batch the chunks\n",
    "batch_size = 9 # free tier limit\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "\n",
    "    # Embed and create FAISS index for this batch\n",
    "    batch_vectorstore = FAISS.from_documents(batch, embeddings)\n",
    "\n",
    "    if vectorstore is None:\n",
    "        vectorstore = batch_vectorstore\n",
    "    else:\n",
    "        vectorstore.merge_from(batch_vectorstore)  # merge new batch into main index\n",
    "\n",
    "    print(f'Processed batch {i // batch_size+1} / {(len(chunks) + batch_size - 1) // batch_size}')\n",
    "\n",
    "    # only sleep if there is another batch\n",
    "    if i + batch_size < len(chunks):\n",
    "        print(\"Sleeping 65 seconds to respect rate limit...\")\n",
    "        time.sleep(65)\n",
    "\n",
    "\n",
    "# Save the FAISS index so it can be reloaded later\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"All chunks processed and stored in FAISS!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore on your own\n",
    "Gemini offers a bigger range of capabilities than those provided here, begin able to automatically handle multi-turn chats is one of them. Explore them on your own!\n",
    "\n",
    "#### Exercise 7\n",
    "Explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a user interface\n",
    "\n",
    "#### Exercise 8\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface).\n",
    "\n",
    "Use a ChatInterface to create a chatbot UI that let's you discuss with Gemini, then add multimodal capabilities for both Gradio and Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Edit the parameters below\n",
    "chats = {}  # store the chat history for each user (suppose multiple users)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
